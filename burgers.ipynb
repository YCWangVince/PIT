{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac064a88-af1a-44b4-88f8-f9720c775157",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from model import dataset\n",
    "from torch.autograd import grad\n",
    "import numpy as np\n",
    "from model.model import VanillaPDETransformer\n",
    "from collections import defaultdict\n",
    "from bcics.boundary_conditions import DirichletBC\n",
    "from bcics.initial_conditions import IC\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76a70e78-dae5-47d1-b48d-9be40475e55a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def relative_l2_error(A, B):\n",
    "    l2_error = torch.norm(A - B)\n",
    "    l2_norm_A = torch.norm(A)\n",
    "    \n",
    "    # To avoid division by zero, add a small constant (e.g., 1e-8)\n",
    "    epsilon = 1e-8\n",
    "    relative_error = l2_error / (l2_norm_A + epsilon)\n",
    "    \n",
    "    return relative_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dafb749-9265-426a-86d9-902875d0b51c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gen_testdata():\n",
    "    data = np.load(\"./data/Burgers.npz\")\n",
    "    t, x, exact = data[\"t\"], data[\"x\"], data[\"usol\"].T\n",
    "    xx, tt = np.meshgrid(x, t)\n",
    "    X = np.vstack((np.ravel(xx), np.ravel(tt))).T\n",
    "    y = exact.flatten()[:, None]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "419b6dcb-2b7d-4f5f-930a-901533a386fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25600, 2)\n",
      "(25600, 1)\n"
     ]
    }
   ],
   "source": [
    "X, y = gen_testdata()\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26b28a40-8bf3-44ee-9be5-c3d8381c99a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pde(x, y):\n",
    "    dy_x = grad(y, x, grad_outputs=torch.ones_like(y), retain_graph=True, create_graph=True)[0][:, 0:1]\n",
    "    dy_t = grad(y, x, grad_outputs=torch.ones_like(y), retain_graph=True, create_graph=True)[0][:, 1:]\n",
    "    dy_xx = grad(dy_x, x, grad_outputs=torch.ones_like(y), retain_graph=True, create_graph=True)[0][:, 0:1]\n",
    "    return dy_t + y * dy_x - 0.01 / np.pi * dy_xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27e9094c-a7da-48c2-8ee4-452489137f87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "geom = [(-1, 1), (0, 0.99)]\n",
    "\n",
    "bc_1 = DirichletBC(geom, boundary_dim=0, boundary_point=-1, time_dim=True, func=lambda x: torch.zeros(x.shape[0]).to(\"cuda\"))\n",
    "bc_2 = DirichletBC(geom, boundary_dim=0, boundary_point=1, time_dim=True, func=lambda x: torch.zeros(x.shape[0]).to(\"cuda\"))\n",
    "ic = IC(geom, lambda x: -torch.sin(np.pi * x[:, 0:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "03176d67-234f-4924-a2da-89a9cd5271ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = defaultdict(lambda: None,\n",
    "                            num_feats=2,\n",
    "                            pos_dim=2,\n",
    "                            n_targets=1,\n",
    "                            n_hidden=64,\n",
    "                            num_feat_layers=2,\n",
    "                            num_encoder_layers=1,\n",
    "                            n_head=4,\n",
    "                            # pred_len=0,\n",
    "                            dim_feedforward=64,\n",
    "                            attention_type='softmax',  # no softmax\n",
    "                            xavier_init=1e-4,\n",
    "                            diagonal_weight=1e-2,\n",
    "                            symmetric_init=False,\n",
    "                            layer_norm=False,\n",
    "                            attn_norm=False,\n",
    "                            batch_norm=True,\n",
    "                            spacial_residual=True,\n",
    "                            return_attn_weight=False,\n",
    "                            seq_len=None,\n",
    "                            activation='silu',\n",
    "                            decoder_type='pointwise',\n",
    "                            # freq_dim=64,\n",
    "                            num_regressor_layers=2,\n",
    "                            # fourier_modes=16,\n",
    "                            spacial_dim=2,\n",
    "                            spacial_fc=True,\n",
    "                            dropout=0.05,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "ee7a9562-663b-475c-a743-b621ad3a8b11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net = VanillaPDETransformer(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "c5559661-22bf-437f-a67a-0d63b6d70c30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pinn_dataset = dataset.pinn_collect_dataset(num_collect=2048, geom=geom, time_dim=True,\n",
    "                 space_distribution='random', time_distribution='uniform', given_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "6a8fd85d-7963-4cc7-8ecb-9864dab3c512",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "collect_data = pinn_dataset.prepare_collection_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "b044a138-cf42-434d-b81e-6114dd2f720f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "boundary_data_1 = np.random.uniform(0, 1, (80, 1))\n",
    "start, end= geom[1]\n",
    "boundary_data_1[:, 0] = boundary_data_1[:, 0] * (end - start) + start\n",
    "new_col = np.full((boundary_data_1.shape[0], 1), -1.)\n",
    "boundary_data_1 = np.hstack((new_col, boundary_data_1))\n",
    "boundary_data_1 = torch.tensor(boundary_data_1, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "996eb346-c302-4a44-8449-31979219ce8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "boundary_data_2 = np.random.uniform(0, 1, (80, 1))\n",
    "start, end= geom[1]\n",
    "boundary_data_2[:, 0] = boundary_data_2[:, 0] * (end - start) + start\n",
    "new_col = np.full((boundary_data_2.shape[0], 1), 1.)\n",
    "boundary_data_2 = np.hstack((new_col, boundary_data_2))\n",
    "boundary_data_2 = torch.tensor(boundary_data_2, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "d346f93c-005b-436c-ad5f-4a77e5857219",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "initial_data = np.random.uniform(0, 1, (160, 1))\n",
    "start, end= geom[0]\n",
    "initial_data[:, 0] = initial_data[:, 0] * (end - start) + start\n",
    "new_col = np.full((initial_data.shape[0], 1), 0.)\n",
    "initial_data = np.hstack((initial_data, new_col))\n",
    "initial_data = torch.tensor(initial_data, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "e9744cc9-d3e7-4d09-b1fa-1f330697d7bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_list = [collect_data,boundary_data_1, boundary_data_2, initial_data]\n",
    "all_data = torch.cat(data_list, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "58664fa4-b54c-4637-b5f4-3fca4fe49216",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sizes = [tensor.size(0) for tensor in data_list]\n",
    "begin_indices = [sum(sizes[:i]) for i in range(len(sizes))]\n",
    "end_indices = [sum(sizes[:i+1]) for i in range(len(sizes))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "14fef404-9d92-4b71-854b-56090e10d3ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2048, 2128, 2208]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "begin_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "d2900934-6539-40b3-92c4-97b1fba7dd5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VanillaPDETransformer(\n",
       "  (dpo): Dropout(p=0.05, inplace=False)\n",
       "  (feat_extract): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=64, bias=True)\n",
       "    (1): SiLU()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): SiLU()\n",
       "    (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (5): SiLU()\n",
       "  )\n",
       "  (encoder_layers): ModuleList(\n",
       "    (0): SimpleTransformerEncoderLayer(\n",
       "      (attn): SimpleAttention(\n",
       "        (linears): ModuleList(\n",
       "          (0-2): 3 x Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (norm_K): ModuleList(\n",
       "          (0-3): 4 x LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (norm_Q): ModuleList(\n",
       "          (0-3): 4 x LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (fc): Linear(in_features=72, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (lr1): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (activation): SiLU()\n",
       "        (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (lr2): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (regressor): PointwiseRegressor(\n",
       "    (fc): Linear(in_features=66, out_features=64, bias=True)\n",
       "    (ff): ModuleList(\n",
       "      (0-1): 2 x Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): SiLU()\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "f0dd7feb-09b2-462d-bdbc-37a95aa841e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_iter_adam = 10000\n",
    "lr = 1e-3\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "MSE = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "35003900-72a3-4cd9-9367-f372fa5ccc0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bc_error_1 = bc_1.error(input_data[begin_indices[1]:end_indices[1]],output[begin_indices[1]:end_indices[1]])\n",
    "loss_bc_1 = MSE(bc_error_1, torch.zeros_like(bc_error_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "72d04945-bf8f-4023-8565-42fcddf60aa5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bc_error_2 = bc_2.error(input_data[begin_indices[2]:end_indices[2]],output[begin_indices[2]:end_indices[2]])\n",
    "loss_bc_2 = MSE(bc_error_2, torch.zeros_like(bc_error_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "25120255-f554-41cb-8198-0c36af5fac5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ic_error = ic.error(input_data[begin_indices[3]:end_indices[3]],output[begin_indices[3]:end_indices[3]])\n",
    "loss_ic = MSE(ic_error, torch.zeros_like(ic_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "63d4bb30-8a38-4d51-bb68-25a3a9dca19d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "class Scheduler(_LRScheduler):\n",
    "    def __init__(self, \n",
    "                 optimizer: Optimizer,\n",
    "                 dim_embed: int,\n",
    "                 warmup_steps: int,\n",
    "                 last_epoch: int=-1,\n",
    "                 verbose: bool=False) -> None:\n",
    "\n",
    "        self.dim_embed = dim_embed\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.num_param_groups = len(optimizer.param_groups)\n",
    "\n",
    "        super().__init__(optimizer, last_epoch, verbose)\n",
    "        \n",
    "    def get_lr(self) -> float:\n",
    "        lr = calc_lr(self._step_count, self.dim_embed, self.warmup_steps)\n",
    "        return [lr] * self.num_param_groups\n",
    "\n",
    "\n",
    "def calc_lr(step, dim_embed, warmup_steps):\n",
    "    return dim_embed**(-0.5) * min(step**(-0.5), step * warmup_steps**(-1.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "fc5b7c68-4e37-44ca-9b08-ce4d3cfe5afd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scheduler = Scheduler(optimizer, 64, 4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "653a66a1-af3a-4071-a6a9-c767983b132a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:100 \t Loss:0.499068 \t Loss_pde:0.000296 \t Loss_bc_1:0.007553 \t Loss_bc_2:0.013723 \t Loss_ic:0.477495\n",
      "Epoch:200 \t Loss:0.466688 \t Loss_pde:0.014335 \t Loss_bc_1:0.021153 \t Loss_bc_2:0.037860 \t Loss_ic:0.393341\n",
      "Epoch:300 \t Loss:0.272905 \t Loss_pde:0.057888 \t Loss_bc_1:0.025469 \t Loss_bc_2:0.031850 \t Loss_ic:0.157698\n",
      "Epoch:400 \t Loss:0.088510 \t Loss_pde:0.031389 \t Loss_bc_1:0.006226 \t Loss_bc_2:0.009535 \t Loss_ic:0.041360\n",
      "Epoch:500 \t Loss:0.049348 \t Loss_pde:0.018090 \t Loss_bc_1:0.004248 \t Loss_bc_2:0.005654 \t Loss_ic:0.021357\n",
      "Epoch:600 \t Loss:0.031040 \t Loss_pde:0.012227 \t Loss_bc_1:0.002239 \t Loss_bc_2:0.003937 \t Loss_ic:0.012637\n",
      "Epoch:700 \t Loss:0.025364 \t Loss_pde:0.009984 \t Loss_bc_1:0.001569 \t Loss_bc_2:0.003844 \t Loss_ic:0.009968\n",
      "Epoch:800 \t Loss:0.021101 \t Loss_pde:0.008605 \t Loss_bc_1:0.001073 \t Loss_bc_2:0.001800 \t Loss_ic:0.009622\n",
      "Epoch:900 \t Loss:0.018566 \t Loss_pde:0.007755 \t Loss_bc_1:0.001186 \t Loss_bc_2:0.001689 \t Loss_ic:0.007935\n",
      "Epoch:1000 \t Loss:0.014207 \t Loss_pde:0.006385 \t Loss_bc_1:0.000596 \t Loss_bc_2:0.000963 \t Loss_ic:0.006264\n",
      "Epoch:1100 \t Loss:0.014995 \t Loss_pde:0.006505 \t Loss_bc_1:0.001357 \t Loss_bc_2:0.000879 \t Loss_ic:0.006253\n",
      "Epoch:1200 \t Loss:0.013751 \t Loss_pde:0.005496 \t Loss_bc_1:0.000437 \t Loss_bc_2:0.000538 \t Loss_ic:0.007280\n",
      "Epoch:1300 \t Loss:0.013379 \t Loss_pde:0.005818 \t Loss_bc_1:0.000439 \t Loss_bc_2:0.000610 \t Loss_ic:0.006510\n",
      "Epoch:1400 \t Loss:0.013846 \t Loss_pde:0.005686 \t Loss_bc_1:0.000443 \t Loss_bc_2:0.000578 \t Loss_ic:0.007139\n",
      "Epoch:1500 \t Loss:0.013259 \t Loss_pde:0.005025 \t Loss_bc_1:0.000453 \t Loss_bc_2:0.000378 \t Loss_ic:0.007403\n",
      "Epoch:1600 \t Loss:0.011196 \t Loss_pde:0.005206 \t Loss_bc_1:0.000408 \t Loss_bc_2:0.000473 \t Loss_ic:0.005107\n",
      "Epoch:1700 \t Loss:0.010978 \t Loss_pde:0.004403 \t Loss_bc_1:0.000282 \t Loss_bc_2:0.000475 \t Loss_ic:0.005818\n",
      "Epoch:1800 \t Loss:0.010019 \t Loss_pde:0.004740 \t Loss_bc_1:0.000763 \t Loss_bc_2:0.000545 \t Loss_ic:0.003970\n",
      "Epoch:1900 \t Loss:0.011318 \t Loss_pde:0.005258 \t Loss_bc_1:0.000294 \t Loss_bc_2:0.000445 \t Loss_ic:0.005322\n",
      "Epoch:2000 \t Loss:0.011691 \t Loss_pde:0.005286 \t Loss_bc_1:0.000347 \t Loss_bc_2:0.000604 \t Loss_ic:0.005454\n",
      "Epoch:2100 \t Loss:0.011047 \t Loss_pde:0.004766 \t Loss_bc_1:0.000329 \t Loss_bc_2:0.000343 \t Loss_ic:0.005609\n",
      "Epoch:2200 \t Loss:0.009740 \t Loss_pde:0.004413 \t Loss_bc_1:0.000356 \t Loss_bc_2:0.000509 \t Loss_ic:0.004462\n",
      "Epoch:2300 \t Loss:0.009049 \t Loss_pde:0.003763 \t Loss_bc_1:0.000246 \t Loss_bc_2:0.000343 \t Loss_ic:0.004697\n",
      "Epoch:2400 \t Loss:0.009018 \t Loss_pde:0.004001 \t Loss_bc_1:0.000313 \t Loss_bc_2:0.000413 \t Loss_ic:0.004292\n",
      "Epoch:2500 \t Loss:0.009380 \t Loss_pde:0.004275 \t Loss_bc_1:0.000300 \t Loss_bc_2:0.000257 \t Loss_ic:0.004548\n",
      "Epoch:2600 \t Loss:0.009326 \t Loss_pde:0.004131 \t Loss_bc_1:0.000514 \t Loss_bc_2:0.000222 \t Loss_ic:0.004459\n",
      "Epoch:2700 \t Loss:0.009367 \t Loss_pde:0.004003 \t Loss_bc_1:0.000321 \t Loss_bc_2:0.000474 \t Loss_ic:0.004569\n",
      "Epoch:2800 \t Loss:0.008541 \t Loss_pde:0.003437 \t Loss_bc_1:0.000290 \t Loss_bc_2:0.000303 \t Loss_ic:0.004511\n",
      "Epoch:2900 \t Loss:0.007962 \t Loss_pde:0.003356 \t Loss_bc_1:0.000501 \t Loss_bc_2:0.000258 \t Loss_ic:0.003846\n",
      "Epoch:3000 \t Loss:0.008463 \t Loss_pde:0.003327 \t Loss_bc_1:0.000276 \t Loss_bc_2:0.000542 \t Loss_ic:0.004318\n",
      "Epoch:3100 \t Loss:0.007615 \t Loss_pde:0.003373 \t Loss_bc_1:0.000168 \t Loss_bc_2:0.000239 \t Loss_ic:0.003835\n",
      "Epoch:3200 \t Loss:0.008231 \t Loss_pde:0.003173 \t Loss_bc_1:0.000171 \t Loss_bc_2:0.000193 \t Loss_ic:0.004693\n",
      "Epoch:3300 \t Loss:0.007689 \t Loss_pde:0.003186 \t Loss_bc_1:0.000378 \t Loss_bc_2:0.000298 \t Loss_ic:0.003827\n",
      "Epoch:3400 \t Loss:0.006621 \t Loss_pde:0.002793 \t Loss_bc_1:0.000307 \t Loss_bc_2:0.000212 \t Loss_ic:0.003309\n",
      "Epoch:3500 \t Loss:0.006123 \t Loss_pde:0.002598 \t Loss_bc_1:0.000179 \t Loss_bc_2:0.000429 \t Loss_ic:0.002917\n",
      "Epoch:3600 \t Loss:0.006661 \t Loss_pde:0.002486 \t Loss_bc_1:0.000225 \t Loss_bc_2:0.000290 \t Loss_ic:0.003660\n",
      "Epoch:3700 \t Loss:0.006611 \t Loss_pde:0.002675 \t Loss_bc_1:0.000193 \t Loss_bc_2:0.000272 \t Loss_ic:0.003471\n",
      "Epoch:3800 \t Loss:0.006560 \t Loss_pde:0.002455 \t Loss_bc_1:0.000286 \t Loss_bc_2:0.000184 \t Loss_ic:0.003635\n",
      "Epoch:3900 \t Loss:0.006096 \t Loss_pde:0.002715 \t Loss_bc_1:0.000164 \t Loss_bc_2:0.000507 \t Loss_ic:0.002710\n",
      "Epoch:4000 \t Loss:0.005782 \t Loss_pde:0.002398 \t Loss_bc_1:0.000172 \t Loss_bc_2:0.000169 \t Loss_ic:0.003043\n",
      "Epoch:4100 \t Loss:0.006552 \t Loss_pde:0.002394 \t Loss_bc_1:0.000138 \t Loss_bc_2:0.000397 \t Loss_ic:0.003623\n",
      "Epoch:4200 \t Loss:0.006384 \t Loss_pde:0.002220 \t Loss_bc_1:0.000364 \t Loss_bc_2:0.000177 \t Loss_ic:0.003623\n",
      "Epoch:4300 \t Loss:0.007123 \t Loss_pde:0.001932 \t Loss_bc_1:0.000128 \t Loss_bc_2:0.000170 \t Loss_ic:0.004892\n",
      "Epoch:4400 \t Loss:0.005927 \t Loss_pde:0.002350 \t Loss_bc_1:0.000126 \t Loss_bc_2:0.000133 \t Loss_ic:0.003318\n",
      "Epoch:4500 \t Loss:0.006552 \t Loss_pde:0.002235 \t Loss_bc_1:0.000175 \t Loss_bc_2:0.000146 \t Loss_ic:0.003996\n",
      "Epoch:4600 \t Loss:0.005508 \t Loss_pde:0.002086 \t Loss_bc_1:0.000185 \t Loss_bc_2:0.000110 \t Loss_ic:0.003127\n",
      "Epoch:4700 \t Loss:0.005988 \t Loss_pde:0.002061 \t Loss_bc_1:0.000164 \t Loss_bc_2:0.000076 \t Loss_ic:0.003686\n",
      "Epoch:4800 \t Loss:0.005026 \t Loss_pde:0.001829 \t Loss_bc_1:0.000295 \t Loss_bc_2:0.000085 \t Loss_ic:0.002816\n",
      "Epoch:4900 \t Loss:0.005364 \t Loss_pde:0.001819 \t Loss_bc_1:0.000158 \t Loss_bc_2:0.000232 \t Loss_ic:0.003155\n",
      "Epoch:5000 \t Loss:0.005032 \t Loss_pde:0.002054 \t Loss_bc_1:0.000177 \t Loss_bc_2:0.000099 \t Loss_ic:0.002701\n",
      "Epoch:5100 \t Loss:0.005012 \t Loss_pde:0.001836 \t Loss_bc_1:0.000151 \t Loss_bc_2:0.000066 \t Loss_ic:0.002960\n",
      "Epoch:5200 \t Loss:0.005847 \t Loss_pde:0.001895 \t Loss_bc_1:0.000190 \t Loss_bc_2:0.000125 \t Loss_ic:0.003637\n",
      "Epoch:5300 \t Loss:0.005142 \t Loss_pde:0.001875 \t Loss_bc_1:0.000369 \t Loss_bc_2:0.000056 \t Loss_ic:0.002842\n",
      "Epoch:5400 \t Loss:0.004912 \t Loss_pde:0.001647 \t Loss_bc_1:0.000177 \t Loss_bc_2:0.000050 \t Loss_ic:0.003039\n",
      "Epoch:5500 \t Loss:0.004484 \t Loss_pde:0.001846 \t Loss_bc_1:0.000138 \t Loss_bc_2:0.000077 \t Loss_ic:0.002422\n",
      "Epoch:5600 \t Loss:0.005202 \t Loss_pde:0.001755 \t Loss_bc_1:0.000248 \t Loss_bc_2:0.000111 \t Loss_ic:0.003088\n",
      "Epoch:5700 \t Loss:0.005087 \t Loss_pde:0.001802 \t Loss_bc_1:0.000153 \t Loss_bc_2:0.000179 \t Loss_ic:0.002953\n",
      "Epoch:5800 \t Loss:0.005051 \t Loss_pde:0.002070 \t Loss_bc_1:0.000134 \t Loss_bc_2:0.000050 \t Loss_ic:0.002798\n",
      "Epoch:5900 \t Loss:0.005042 \t Loss_pde:0.001708 \t Loss_bc_1:0.000267 \t Loss_bc_2:0.000058 \t Loss_ic:0.003008\n",
      "Epoch:6000 \t Loss:0.004972 \t Loss_pde:0.001702 \t Loss_bc_1:0.000311 \t Loss_bc_2:0.000069 \t Loss_ic:0.002890\n",
      "Epoch:6100 \t Loss:0.005329 \t Loss_pde:0.001950 \t Loss_bc_1:0.000427 \t Loss_bc_2:0.000131 \t Loss_ic:0.002821\n",
      "Epoch:6200 \t Loss:0.004304 \t Loss_pde:0.001917 \t Loss_bc_1:0.000098 \t Loss_bc_2:0.000062 \t Loss_ic:0.002226\n",
      "Epoch:6300 \t Loss:0.004915 \t Loss_pde:0.001873 \t Loss_bc_1:0.000157 \t Loss_bc_2:0.000065 \t Loss_ic:0.002820\n",
      "Epoch:6400 \t Loss:0.003719 \t Loss_pde:0.001481 \t Loss_bc_1:0.000100 \t Loss_bc_2:0.000073 \t Loss_ic:0.002065\n",
      "Epoch:6500 \t Loss:0.004887 \t Loss_pde:0.001770 \t Loss_bc_1:0.000099 \t Loss_bc_2:0.000068 \t Loss_ic:0.002949\n",
      "Epoch:6600 \t Loss:0.004343 \t Loss_pde:0.001672 \t Loss_bc_1:0.000067 \t Loss_bc_2:0.000073 \t Loss_ic:0.002531\n",
      "Epoch:6700 \t Loss:0.004413 \t Loss_pde:0.001572 \t Loss_bc_1:0.000088 \t Loss_bc_2:0.000056 \t Loss_ic:0.002697\n",
      "Epoch:6800 \t Loss:0.004187 \t Loss_pde:0.001548 \t Loss_bc_1:0.000084 \t Loss_bc_2:0.000076 \t Loss_ic:0.002479\n",
      "Epoch:6900 \t Loss:0.004021 \t Loss_pde:0.001571 \t Loss_bc_1:0.000098 \t Loss_bc_2:0.000068 \t Loss_ic:0.002284\n",
      "Epoch:7000 \t Loss:0.004493 \t Loss_pde:0.001554 \t Loss_bc_1:0.000149 \t Loss_bc_2:0.000125 \t Loss_ic:0.002664\n",
      "Epoch:7100 \t Loss:0.004417 \t Loss_pde:0.001706 \t Loss_bc_1:0.000078 \t Loss_bc_2:0.000073 \t Loss_ic:0.002560\n",
      "Epoch:7200 \t Loss:0.004272 \t Loss_pde:0.001393 \t Loss_bc_1:0.000175 \t Loss_bc_2:0.000100 \t Loss_ic:0.002604\n",
      "Epoch:7300 \t Loss:0.004188 \t Loss_pde:0.001577 \t Loss_bc_1:0.000098 \t Loss_bc_2:0.000128 \t Loss_ic:0.002385\n",
      "Epoch:7400 \t Loss:0.004170 \t Loss_pde:0.001411 \t Loss_bc_1:0.000121 \t Loss_bc_2:0.000040 \t Loss_ic:0.002598\n",
      "Epoch:7500 \t Loss:0.004219 \t Loss_pde:0.001560 \t Loss_bc_1:0.000101 \t Loss_bc_2:0.000037 \t Loss_ic:0.002522\n",
      "Epoch:7600 \t Loss:0.004108 \t Loss_pde:0.001489 \t Loss_bc_1:0.000090 \t Loss_bc_2:0.000057 \t Loss_ic:0.002472\n",
      "Epoch:7700 \t Loss:0.003401 \t Loss_pde:0.001260 \t Loss_bc_1:0.000104 \t Loss_bc_2:0.000038 \t Loss_ic:0.001999\n",
      "Epoch:7800 \t Loss:0.004186 \t Loss_pde:0.001615 \t Loss_bc_1:0.000117 \t Loss_bc_2:0.000036 \t Loss_ic:0.002418\n",
      "Epoch:7900 \t Loss:0.004011 \t Loss_pde:0.001617 \t Loss_bc_1:0.000100 \t Loss_bc_2:0.000031 \t Loss_ic:0.002263\n",
      "Epoch:8000 \t Loss:0.004139 \t Loss_pde:0.001423 \t Loss_bc_1:0.000060 \t Loss_bc_2:0.000035 \t Loss_ic:0.002620\n",
      "Epoch:8100 \t Loss:0.003586 \t Loss_pde:0.001365 \t Loss_bc_1:0.000176 \t Loss_bc_2:0.000055 \t Loss_ic:0.001990\n",
      "Epoch:8200 \t Loss:0.004381 \t Loss_pde:0.001303 \t Loss_bc_1:0.000089 \t Loss_bc_2:0.000026 \t Loss_ic:0.002963\n",
      "Epoch:8300 \t Loss:0.003845 \t Loss_pde:0.001382 \t Loss_bc_1:0.000063 \t Loss_bc_2:0.000041 \t Loss_ic:0.002360\n",
      "Epoch:8400 \t Loss:0.003645 \t Loss_pde:0.001375 \t Loss_bc_1:0.000107 \t Loss_bc_2:0.000040 \t Loss_ic:0.002123\n",
      "Epoch:8500 \t Loss:0.003546 \t Loss_pde:0.001237 \t Loss_bc_1:0.000080 \t Loss_bc_2:0.000054 \t Loss_ic:0.002175\n",
      "Epoch:8600 \t Loss:0.004104 \t Loss_pde:0.001400 \t Loss_bc_1:0.000075 \t Loss_bc_2:0.000052 \t Loss_ic:0.002577\n",
      "Epoch:8700 \t Loss:0.003954 \t Loss_pde:0.001383 \t Loss_bc_1:0.000072 \t Loss_bc_2:0.000045 \t Loss_ic:0.002454\n",
      "Epoch:8800 \t Loss:0.003166 \t Loss_pde:0.001373 \t Loss_bc_1:0.000063 \t Loss_bc_2:0.000038 \t Loss_ic:0.001693\n",
      "Epoch:8900 \t Loss:0.004052 \t Loss_pde:0.001615 \t Loss_bc_1:0.000065 \t Loss_bc_2:0.000076 \t Loss_ic:0.002296\n",
      "Epoch:9000 \t Loss:0.004050 \t Loss_pde:0.001534 \t Loss_bc_1:0.000372 \t Loss_bc_2:0.000076 \t Loss_ic:0.002069\n",
      "Epoch:9100 \t Loss:0.003576 \t Loss_pde:0.001473 \t Loss_bc_1:0.000064 \t Loss_bc_2:0.000026 \t Loss_ic:0.002012\n",
      "Epoch:9200 \t Loss:0.004219 \t Loss_pde:0.001495 \t Loss_bc_1:0.000177 \t Loss_bc_2:0.000026 \t Loss_ic:0.002521\n",
      "Epoch:9300 \t Loss:0.003993 \t Loss_pde:0.001316 \t Loss_bc_1:0.000082 \t Loss_bc_2:0.000336 \t Loss_ic:0.002258\n",
      "Epoch:9400 \t Loss:0.003597 \t Loss_pde:0.001311 \t Loss_bc_1:0.000070 \t Loss_bc_2:0.000042 \t Loss_ic:0.002173\n",
      "Epoch:9500 \t Loss:0.003972 \t Loss_pde:0.001346 \t Loss_bc_1:0.000081 \t Loss_bc_2:0.000041 \t Loss_ic:0.002505\n",
      "Epoch:9600 \t Loss:0.003952 \t Loss_pde:0.001421 \t Loss_bc_1:0.000054 \t Loss_bc_2:0.000068 \t Loss_ic:0.002409\n",
      "Epoch:9700 \t Loss:0.003982 \t Loss_pde:0.001436 \t Loss_bc_1:0.000062 \t Loss_bc_2:0.000052 \t Loss_ic:0.002432\n",
      "Epoch:9800 \t Loss:0.003828 \t Loss_pde:0.001278 \t Loss_bc_1:0.000190 \t Loss_bc_2:0.000051 \t Loss_ic:0.002309\n",
      "Epoch:9900 \t Loss:0.004212 \t Loss_pde:0.001485 \t Loss_bc_1:0.000093 \t Loss_bc_2:0.000074 \t Loss_ic:0.002559\n",
      "Epoch:10000 \t Loss:0.003334 \t Loss_pde:0.001268 \t Loss_bc_1:0.000098 \t Loss_bc_2:0.000038 \t Loss_ic:0.001931\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, max_iter_adam+1):\n",
    "    input_data = all_data.to('cuda:0') \n",
    "    input_data.requires_grad=True\n",
    "    output = net(input_data)\n",
    "    pde_results = pde(input_data, output)\n",
    "    loss_res = MSE(pde_results, torch.zeros_like(pde_results))\n",
    "    \n",
    "    bc_error_1 = bc_1.error(input_data[begin_indices[1]:end_indices[1]],output[begin_indices[1]:end_indices[1]])\n",
    "    loss_bc_1 = MSE(bc_error_1, torch.zeros_like(bc_error_1))\n",
    "    \n",
    "    bc_error_2 = bc_2.error(input_data[begin_indices[2]:end_indices[2]],output[begin_indices[2]:end_indices[2]])\n",
    "    loss_bc_2 = MSE(bc_error_2, torch.zeros_like(bc_error_2))\n",
    "    \n",
    "    ic_error = ic.error(input_data[begin_indices[3]:end_indices[3]],output[begin_indices[3]:end_indices[3]])\n",
    "    loss_ic = MSE(ic_error, torch.zeros_like(ic_error))\n",
    "    \n",
    "    loss= loss_res+loss_bc_1+loss_bc_2+loss_ic\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    if i%100 ==0:\n",
    "        print('Epoch:%d \\t Loss:%.6f \\t Loss_pde:%.6f \\t Loss_bc_1:%.6f \\t Loss_bc_2:%.6f \\t Loss_ic:%.6f'%(i,loss.detach().cpu().numpy(), loss_res.detach().cpu().numpy(), loss_bc_1.detach().cpu().numpy(), loss_bc_2.detach().cpu().numpy(), loss_ic.detach().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "9bc09837-dc14-436d-be07-9a8b4e39e9ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net.eval()\n",
    "test_data = torch.as_tensor(X,dtype=torch.float32).to('cuda:0')\n",
    "test_label = torch.as_tensor(y,dtype=torch.float32).to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "e8f1230e-81d0-4da9-9621-0308a429b6af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    test_pred = net(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "c98905a7-0254-4361-a37c-0d9f650beba9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = relative_l2_error(test_label, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "2119b606-97f0-4246-9af3-e82b8d3a5585",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3188, device='cuda:0')"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "aa201d33-fb53-422a-a366-1c9a54ebefaa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VanillaPDETransformer(\n",
       "  (dpo): Dropout(p=0.05, inplace=False)\n",
       "  (feat_extract): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=64, bias=True)\n",
       "    (1): SiLU()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): SiLU()\n",
       "    (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (5): SiLU()\n",
       "  )\n",
       "  (encoder_layers): ModuleList(\n",
       "    (0): SimpleTransformerEncoderLayer(\n",
       "      (attn): SimpleAttention(\n",
       "        (linears): ModuleList(\n",
       "          (0-2): 3 x Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (norm_K): ModuleList(\n",
       "          (0-3): 4 x LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (norm_Q): ModuleList(\n",
       "          (0-3): 4 x LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (fc): Linear(in_features=72, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (lr1): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (activation): SiLU()\n",
       "        (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (lr2): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (regressor): PointwiseRegressor(\n",
       "    (fc): Linear(in_features=66, out_features=64, bias=True)\n",
       "    (ff): ModuleList(\n",
       "      (0-1): 2 x Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): SiLU()\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "21fe7896-f4c2-4547-8146-4fa0d713d080",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer_lbfgs = torch.optim.LBFGS(\n",
    "            net.parameters(), \n",
    "            lr=1.0, \n",
    "            max_iter=10000, \n",
    "            max_eval=None, \n",
    "            history_size=100,\n",
    "            tolerance_grad=1e-5, \n",
    "            tolerance_change=1.0 * np.finfo(float).eps,\n",
    "            line_search_fn=\"strong_wolfe\"       # can be \"strong_wolfe\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "44d26af5-87ac-4777-9b10-cba6cd82cd85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def closure():\n",
    "    optimizer_lbfgs.zero_grad()\n",
    "    input_data = all_data.to('cuda:0') \n",
    "    input_data.requires_grad=True\n",
    "    output = net(input_data)\n",
    "    pde_results = pde(input_data, output)\n",
    "    loss_res = MSE(pde_results, torch.zeros_like(pde_results))\n",
    "    \n",
    "    bc_error_1 = bc_1.error(input_data[begin_indices[1]:end_indices[1]],output[begin_indices[1]:end_indices[1]])\n",
    "    loss_bc_1 = MSE(bc_error_1, torch.zeros_like(bc_error_1))\n",
    "    \n",
    "    bc_error_2 = bc_2.error(input_data[begin_indices[2]:end_indices[2]],output[begin_indices[2]:end_indices[2]])\n",
    "    loss_bc_2 = MSE(bc_error_2, torch.zeros_like(bc_error_2))\n",
    "    \n",
    "    ic_error = ic.error(input_data[begin_indices[3]:end_indices[3]],output[begin_indices[3]:end_indices[3]])\n",
    "    loss_ic = MSE(ic_error, torch.zeros_like(ic_error))\n",
    "    \n",
    "    loss= loss_res+loss_bc_1+loss_bc_2+loss_ic\n",
    "    if i%100==0:\n",
    "        print('Epoch:%d \\t Loss:%.6f \\t Loss_pde:%.6f \\t Loss_bc_1:%.6f \\t Loss_bc_2:%.6f \\t Loss_ic:%.6f'%(i,loss.detach().cpu().numpy(), loss_res.detach().cpu().numpy(), loss_bc_1.detach().cpu().numpy(), loss_bc_2.detach().cpu().numpy(), loss_ic.detach().cpu().numpy()))\n",
    "    loss.backward()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "62faf9c1-f9f3-4c46-b06e-982f37091814",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:100 \t Loss:0.003811 \t Loss_pde:0.001277 \t Loss_bc_1:0.000066 \t Loss_bc_2:0.000027 \t Loss_ic:0.002441\n",
      "Epoch:100 \t Loss:0.004065 \t Loss_pde:0.001306 \t Loss_bc_1:0.000084 \t Loss_bc_2:0.000030 \t Loss_ic:0.002646\n",
      "Epoch:100 \t Loss:0.002793 \t Loss_pde:0.001109 \t Loss_bc_1:0.000054 \t Loss_bc_2:0.000028 \t Loss_ic:0.001601\n",
      "Epoch:100 \t Loss:0.003278 \t Loss_pde:0.001342 \t Loss_bc_1:0.000079 \t Loss_bc_2:0.000023 \t Loss_ic:0.001835\n",
      "Epoch:100 \t Loss:0.003479 \t Loss_pde:0.001349 \t Loss_bc_1:0.000054 \t Loss_bc_2:0.000027 \t Loss_ic:0.002049\n",
      "Epoch:200 \t Loss:0.003803 \t Loss_pde:0.001214 \t Loss_bc_1:0.000052 \t Loss_bc_2:0.000028 \t Loss_ic:0.002509\n",
      "Epoch:200 \t Loss:0.003384 \t Loss_pde:0.001366 \t Loss_bc_1:0.000062 \t Loss_bc_2:0.000032 \t Loss_ic:0.001924\n",
      "Epoch:200 \t Loss:0.003388 \t Loss_pde:0.001269 \t Loss_bc_1:0.000065 \t Loss_bc_2:0.000030 \t Loss_ic:0.002024\n",
      "Epoch:200 \t Loss:0.003508 \t Loss_pde:0.001275 \t Loss_bc_1:0.000062 \t Loss_bc_2:0.000026 \t Loss_ic:0.002145\n",
      "Epoch:200 \t Loss:0.003531 \t Loss_pde:0.001315 \t Loss_bc_1:0.000064 \t Loss_bc_2:0.000028 \t Loss_ic:0.002123\n",
      "Epoch:200 \t Loss:0.003667 \t Loss_pde:0.001265 \t Loss_bc_1:0.000068 \t Loss_bc_2:0.000030 \t Loss_ic:0.002304\n",
      "Epoch:300 \t Loss:0.003329 \t Loss_pde:0.001256 \t Loss_bc_1:0.000046 \t Loss_bc_2:0.000030 \t Loss_ic:0.001997\n",
      "Epoch:300 \t Loss:0.003201 \t Loss_pde:0.001347 \t Loss_bc_1:0.000054 \t Loss_bc_2:0.000024 \t Loss_ic:0.001776\n",
      "Epoch:300 \t Loss:0.003456 \t Loss_pde:0.001249 \t Loss_bc_1:0.000050 \t Loss_bc_2:0.000030 \t Loss_ic:0.002128\n",
      "Epoch:300 \t Loss:0.003633 \t Loss_pde:0.001403 \t Loss_bc_1:0.000048 \t Loss_bc_2:0.000025 \t Loss_ic:0.002158\n",
      "Epoch:300 \t Loss:0.003573 \t Loss_pde:0.001283 \t Loss_bc_1:0.000072 \t Loss_bc_2:0.000024 \t Loss_ic:0.002195\n",
      "Epoch:300 \t Loss:0.003407 \t Loss_pde:0.001225 \t Loss_bc_1:0.000077 \t Loss_bc_2:0.000019 \t Loss_ic:0.002086\n",
      "Epoch:400 \t Loss:0.003275 \t Loss_pde:0.001212 \t Loss_bc_1:0.000047 \t Loss_bc_2:0.000028 \t Loss_ic:0.001987\n",
      "Epoch:400 \t Loss:0.003348 \t Loss_pde:0.001268 \t Loss_bc_1:0.000042 \t Loss_bc_2:0.000027 \t Loss_ic:0.002011\n",
      "Epoch:400 \t Loss:0.003443 \t Loss_pde:0.001325 \t Loss_bc_1:0.000067 \t Loss_bc_2:0.000019 \t Loss_ic:0.002031\n",
      "Epoch:400 \t Loss:0.003346 \t Loss_pde:0.001190 \t Loss_bc_1:0.000089 \t Loss_bc_2:0.000023 \t Loss_ic:0.002045\n",
      "Epoch:400 \t Loss:0.003349 \t Loss_pde:0.001308 \t Loss_bc_1:0.000075 \t Loss_bc_2:0.000032 \t Loss_ic:0.001934\n",
      "Epoch:500 \t Loss:0.003727 \t Loss_pde:0.001394 \t Loss_bc_1:0.000059 \t Loss_bc_2:0.000036 \t Loss_ic:0.002239\n",
      "Epoch:500 \t Loss:0.003473 \t Loss_pde:0.001292 \t Loss_bc_1:0.000064 \t Loss_bc_2:0.000026 \t Loss_ic:0.002090\n",
      "Epoch:500 \t Loss:0.003517 \t Loss_pde:0.001256 \t Loss_bc_1:0.000074 \t Loss_bc_2:0.000027 \t Loss_ic:0.002160\n",
      "Epoch:500 \t Loss:0.003468 \t Loss_pde:0.001397 \t Loss_bc_1:0.000057 \t Loss_bc_2:0.000029 \t Loss_ic:0.001984\n",
      "Epoch:500 \t Loss:0.003619 \t Loss_pde:0.001352 \t Loss_bc_1:0.000085 \t Loss_bc_2:0.000024 \t Loss_ic:0.002158\n",
      "Epoch:500 \t Loss:0.003462 \t Loss_pde:0.001236 \t Loss_bc_1:0.000051 \t Loss_bc_2:0.000025 \t Loss_ic:0.002150\n",
      "Epoch:500 \t Loss:0.003667 \t Loss_pde:0.001404 \t Loss_bc_1:0.000042 \t Loss_bc_2:0.000028 \t Loss_ic:0.002194\n",
      "Epoch:500 \t Loss:0.003452 \t Loss_pde:0.001198 \t Loss_bc_1:0.000052 \t Loss_bc_2:0.000020 \t Loss_ic:0.002181\n",
      "Epoch:500 \t Loss:0.003722 \t Loss_pde:0.001453 \t Loss_bc_1:0.000070 \t Loss_bc_2:0.000027 \t Loss_ic:0.002172\n",
      "Epoch:500 \t Loss:0.003475 \t Loss_pde:0.001376 \t Loss_bc_1:0.000074 \t Loss_bc_2:0.000029 \t Loss_ic:0.001996\n",
      "Epoch:500 \t Loss:0.003489 \t Loss_pde:0.001310 \t Loss_bc_1:0.000061 \t Loss_bc_2:0.000020 \t Loss_ic:0.002098\n",
      "Epoch:500 \t Loss:0.003328 \t Loss_pde:0.001350 \t Loss_bc_1:0.000058 \t Loss_bc_2:0.000027 \t Loss_ic:0.001893\n",
      "Epoch:500 \t Loss:0.003495 \t Loss_pde:0.001271 \t Loss_bc_1:0.000064 \t Loss_bc_2:0.000025 \t Loss_ic:0.002134\n",
      "Epoch:500 \t Loss:0.003151 \t Loss_pde:0.001237 \t Loss_bc_1:0.000070 \t Loss_bc_2:0.000028 \t Loss_ic:0.001817\n",
      "Epoch:500 \t Loss:0.003463 \t Loss_pde:0.001310 \t Loss_bc_1:0.000067 \t Loss_bc_2:0.000024 \t Loss_ic:0.002061\n",
      "Epoch:500 \t Loss:0.003407 \t Loss_pde:0.001372 \t Loss_bc_1:0.000045 \t Loss_bc_2:0.000026 \t Loss_ic:0.001965\n",
      "Epoch:500 \t Loss:0.003247 \t Loss_pde:0.001311 \t Loss_bc_1:0.000053 \t Loss_bc_2:0.000025 \t Loss_ic:0.001858\n",
      "Epoch:500 \t Loss:0.003497 \t Loss_pde:0.001297 \t Loss_bc_1:0.000073 \t Loss_bc_2:0.000031 \t Loss_ic:0.002096\n",
      "Epoch:600 \t Loss:0.003674 \t Loss_pde:0.001412 \t Loss_bc_1:0.000060 \t Loss_bc_2:0.000028 \t Loss_ic:0.002173\n",
      "Epoch:600 \t Loss:0.003731 \t Loss_pde:0.001252 \t Loss_bc_1:0.000054 \t Loss_bc_2:0.000027 \t Loss_ic:0.002398\n",
      "Epoch:600 \t Loss:0.003776 \t Loss_pde:0.001392 \t Loss_bc_1:0.000076 \t Loss_bc_2:0.000035 \t Loss_ic:0.002273\n",
      "Epoch:600 \t Loss:0.004460 \t Loss_pde:0.001244 \t Loss_bc_1:0.000075 \t Loss_bc_2:0.000026 \t Loss_ic:0.003115\n",
      "Epoch:600 \t Loss:0.003348 \t Loss_pde:0.001313 \t Loss_bc_1:0.000063 \t Loss_bc_2:0.000019 \t Loss_ic:0.001952\n",
      "Epoch:600 \t Loss:0.003707 \t Loss_pde:0.001307 \t Loss_bc_1:0.000061 \t Loss_bc_2:0.000026 \t Loss_ic:0.002313\n",
      "Epoch:600 \t Loss:0.003476 \t Loss_pde:0.001247 \t Loss_bc_1:0.000048 \t Loss_bc_2:0.000039 \t Loss_ic:0.002143\n",
      "Epoch:600 \t Loss:0.003562 \t Loss_pde:0.001218 \t Loss_bc_1:0.000047 \t Loss_bc_2:0.000034 \t Loss_ic:0.002263\n",
      "Epoch:600 \t Loss:0.003471 \t Loss_pde:0.001301 \t Loss_bc_1:0.000055 \t Loss_bc_2:0.000024 \t Loss_ic:0.002091\n",
      "Epoch:700 \t Loss:0.003444 \t Loss_pde:0.001385 \t Loss_bc_1:0.000047 \t Loss_bc_2:0.000019 \t Loss_ic:0.001993\n",
      "Epoch:700 \t Loss:0.003196 \t Loss_pde:0.001187 \t Loss_bc_1:0.000051 \t Loss_bc_2:0.000017 \t Loss_ic:0.001941\n",
      "Epoch:700 \t Loss:0.003616 \t Loss_pde:0.001203 \t Loss_bc_1:0.000072 \t Loss_bc_2:0.000018 \t Loss_ic:0.002322\n",
      "Epoch:700 \t Loss:0.003468 \t Loss_pde:0.001321 \t Loss_bc_1:0.000057 \t Loss_bc_2:0.000019 \t Loss_ic:0.002071\n",
      "Epoch:700 \t Loss:0.003952 \t Loss_pde:0.001303 \t Loss_bc_1:0.000053 \t Loss_bc_2:0.000021 \t Loss_ic:0.002575\n",
      "Epoch:700 \t Loss:0.004144 \t Loss_pde:0.001278 \t Loss_bc_1:0.000060 \t Loss_bc_2:0.000028 \t Loss_ic:0.002777\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[267], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10000\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[43moptimizer_lbfgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pit/lib/python3.8/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pit/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pit/lib/python3.8/site-packages/torch/optim/lbfgs.py:426\u001b[0m, in \u001b[0;36mLBFGS.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobj_func\u001b[39m(x, t, d):\n\u001b[1;32m    424\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_directional_evaluate(closure, x, t, d)\n\u001b[0;32m--> 426\u001b[0m     loss, flat_grad, t, ls_func_evals \u001b[38;5;241m=\u001b[39m \u001b[43m_strong_wolfe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgtd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_grad(t, d)\n\u001b[1;32m    429\u001b[0m opt_cond \u001b[38;5;241m=\u001b[39m flat_grad\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m tolerance_grad\n",
      "File \u001b[0;32m~/anaconda3/envs/pit/lib/python3.8/site-packages/torch/optim/lbfgs.py:99\u001b[0m, in \u001b[0;36m_strong_wolfe\u001b[0;34m(obj_func, x, t, d, f, g, gtd, c1, c2, tolerance_change, max_ls)\u001b[0m\n\u001b[1;32m     97\u001b[0m g_prev \u001b[38;5;241m=\u001b[39m g_new\u001b[38;5;241m.\u001b[39mclone(memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcontiguous_format)\n\u001b[1;32m     98\u001b[0m gtd_prev \u001b[38;5;241m=\u001b[39m gtd_new\n\u001b[0;32m---> 99\u001b[0m f_new, g_new \u001b[38;5;241m=\u001b[39m \u001b[43mobj_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m ls_func_evals \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    101\u001b[0m gtd_new \u001b[38;5;241m=\u001b[39m g_new\u001b[38;5;241m.\u001b[39mdot(d)\n",
      "File \u001b[0;32m~/anaconda3/envs/pit/lib/python3.8/site-packages/torch/optim/lbfgs.py:424\u001b[0m, in \u001b[0;36mLBFGS.step.<locals>.obj_func\u001b[0;34m(x, t, d)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobj_func\u001b[39m(x, t, d):\n\u001b[0;32m--> 424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_directional_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pit/lib/python3.8/site-packages/torch/optim/lbfgs.py:278\u001b[0m, in \u001b[0;36mLBFGS._directional_evaluate\u001b[0;34m(self, closure, x, t, d)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_directional_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, closure, x, t, d):\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_grad(t, d)\n\u001b[0;32m--> 278\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    279\u001b[0m     flat_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gather_flat_grad()\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_param(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/pit/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[266], line 21\u001b[0m, in \u001b[0;36mclosure\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch:\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m Loss:\u001b[39m\u001b[38;5;132;01m%.6f\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m Loss_pde:\u001b[39m\u001b[38;5;132;01m%.6f\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m Loss_bc_1:\u001b[39m\u001b[38;5;132;01m%.6f\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m Loss_bc_2:\u001b[39m\u001b[38;5;132;01m%.6f\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m Loss_ic:\u001b[39m\u001b[38;5;132;01m%.6f\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m%\u001b[39m(i,loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), loss_res\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), loss_bc_1\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), loss_bc_2\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), loss_ic\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()))\n\u001b[0;32m---> 21\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/pit/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pit/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(1, 10000+1):\n",
    "    optimizer_lbfgs.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379e8901-a5d6-49da-9fec-1760f2ddcc5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
