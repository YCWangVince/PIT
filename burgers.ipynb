{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac064a88-af1a-44b4-88f8-f9720c775157",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from model import dataset\n",
    "from torch.autograd import grad\n",
    "import numpy as np\n",
    "from model.model import PDETransformer\n",
    "from collections import defaultdict\n",
    "from bcics.boundary_conditions import DirichletBC\n",
    "from bcics.initial_conditions import IC\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76a70e78-dae5-47d1-b48d-9be40475e55a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def relative_l2_error(A, B):\n",
    "    l2_error = torch.norm(A - B)\n",
    "    l2_norm_A = torch.norm(A)\n",
    "    \n",
    "    # To avoid division by zero, add a small constant (e.g., 1e-8)\n",
    "    epsilon = 1e-8\n",
    "    relative_error = l2_error / (l2_norm_A + epsilon)\n",
    "    \n",
    "    return relative_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4dafb749-9265-426a-86d9-902875d0b51c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gen_testdata():\n",
    "    data = np.load(\"./data/Burgers.npz\")\n",
    "    t, x, exact = data[\"t\"], data[\"x\"], data[\"usol\"].T\n",
    "    xx, tt = np.meshgrid(x, t)\n",
    "    X = np.vstack((np.ravel(xx), np.ravel(tt))).T\n",
    "    y = exact.flatten()[:, None]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "419b6dcb-2b7d-4f5f-930a-901533a386fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X, y = gen_testdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f5444f9-fb1a-44d2-b86f-f724ed8f4be6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "select_index = np.random.randint(X.shape[0], size=2048)\n",
    "# X_train = torch.tensor(X[select_index, :],dtype=torch.float32)\n",
    "# y_train = torch.tensor(y[select_index, :],dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26b28a40-8bf3-44ee-9be5-c3d8381c99a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pde(x, y):\n",
    "    dy_x = grad(y, x, grad_outputs=torch.ones_like(y), retain_graph=True, create_graph=True)[0][:, :, 0:1]\n",
    "    dy_t = grad(y, x, grad_outputs=torch.ones_like(y), retain_graph=True, create_graph=True)[0][:, :, 1:]\n",
    "    dy_xx = grad(dy_x, x, grad_outputs=torch.ones_like(y), retain_graph=True, create_graph=True)[0][:, :, 0:1]\n",
    "    return dy_t + y * dy_x - 0.01 / np.pi * dy_xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27e9094c-a7da-48c2-8ee4-452489137f87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "geom = [(-1, 1), (0, 0.99)]\n",
    "\n",
    "bc_1 = DirichletBC(geom, boundary_dim=0, boundary_point=-1, time_dim=True, func=lambda x: torch.zeros(list(x.shape[:-1])+[1]).to(\"cuda\"))\n",
    "bc_2 = DirichletBC(geom, boundary_dim=0, boundary_point=1, time_dim=True, func=lambda x: torch.zeros(list(x.shape[:-1])+[1]).to(\"cuda\"))\n",
    "ic = IC(geom, lambda x: -torch.sin(np.pi * x[:, :, 0:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "03176d67-234f-4924-a2da-89a9cd5271ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = defaultdict(num_dim=2,\n",
    "                         n_targets=1,\n",
    "                         n_hidden=64,\n",
    "                         num_feat_layers=3,\n",
    "                         num_encoder_layers=2,\n",
    "                         activation='tanh',\n",
    "                         n_head=2,\n",
    "                         dim_feedforward=64,\n",
    "                         attention_type='fourier',  # no softmax\n",
    "                         layer_norm=False,\n",
    "                         attn_norm=False,\n",
    "                         attn_norm_type='layer',\n",
    "                         batch_norm=True,\n",
    "                         spacial_residual=True,\n",
    "                         num_regressor_layers=3,\n",
    "                         spacial_fc=False,\n",
    "                         return_attn_weight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c5559661-22bf-437f-a67a-0d63b6d70c30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pinn_dataset = dataset.pinn_collect_dataset(num_collect=2048, geom=geom, time_dim=True,\n",
    "                 distribution='random', given_data=False)\n",
    "collect_data = pinn_dataset.prepare_collection_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b044a138-cf42-434d-b81e-6114dd2f720f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "collect_data = pinn_dataset.prepare_collection_data()\n",
    "\n",
    "boundary_data_1 = np.random.uniform(0, 1, (80, 1))\n",
    "start, end= geom[1]\n",
    "boundary_data_1[:, 0] = boundary_data_1[:, 0] * (end - start) + start\n",
    "new_col = np.full((boundary_data_1.shape[0], 1), -1.)\n",
    "boundary_data_1 = np.hstack((new_col, boundary_data_1))\n",
    "boundary_data_1 = torch.tensor(boundary_data_1, dtype=torch.float32)\n",
    "\n",
    "boundary_data_2 = np.random.uniform(0, 1, (80, 1))\n",
    "start, end= geom[1]\n",
    "boundary_data_2[:, 0] = boundary_data_2[:, 0] * (end - start) + start\n",
    "new_col = np.full((boundary_data_2.shape[0], 1), 1.)\n",
    "boundary_data_2 = np.hstack((new_col, boundary_data_2))\n",
    "boundary_data_2 = torch.tensor(boundary_data_2, dtype=torch.float32)\n",
    "\n",
    "initial_data = np.random.uniform(0, 1, (160, 1))\n",
    "start, end= geom[0]\n",
    "initial_data[:, 0] = initial_data[:, 0] * (end - start) + start\n",
    "new_col = np.full((initial_data.shape[0], 1), 0.)\n",
    "initial_data = np.hstack((initial_data, new_col))\n",
    "initial_data = torch.tensor(initial_data, dtype=torch.float32)\n",
    "\n",
    "data_list = [collect_data,boundary_data_1, boundary_data_2, initial_data]\n",
    "# data_list = [X_train,boundary_data_1, boundary_data_2, initial_data]\n",
    "all_data = torch.cat(data_list, dim=0)\n",
    "\n",
    "\n",
    "sizes = [tensor.size(0) for tensor in data_list]\n",
    "begin_indices = [sum(sizes[:i]) for i in range(len(sizes))]\n",
    "end_indices = [sum(sizes[:i+1]) for i in range(len(sizes))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "996eb346-c302-4a44-8449-31979219ce8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "boundary_data_2 = np.random.uniform(0, 1, (80, 1))\n",
    "start, end= geom[1]\n",
    "boundary_data_2[:, 0] = boundary_data_2[:, 0] * (end - start) + start\n",
    "new_col = np.full((boundary_data_2.shape[0], 1), 1.)\n",
    "boundary_data_2 = np.hstack((new_col, boundary_data_2))\n",
    "boundary_data_2 = torch.tensor(boundary_data_2, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d346f93c-005b-436c-ad5f-4a77e5857219",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "initial_data = np.random.uniform(0, 1, (160, 1))\n",
    "start, end= geom[0]\n",
    "initial_data[:, 0] = initial_data[:, 0] * (end - start) + start\n",
    "new_col = np.full((initial_data.shape[0], 1), 0.)\n",
    "initial_data = np.hstack((initial_data, new_col))\n",
    "initial_data = torch.tensor(initial_data, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e9744cc9-d3e7-4d09-b1fa-1f330697d7bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_list = [collect_data,boundary_data_1, boundary_data_2, initial_data]\n",
    "# data_list = [X_train,boundary_data_1, boundary_data_2, initial_data]\n",
    "all_data = torch.cat(data_list, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "58664fa4-b54c-4637-b5f4-3fca4fe49216",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sizes = [tensor.size(0) for tensor in data_list]\n",
    "begin_indices = [sum(sizes[:i]) for i in range(len(sizes))]\n",
    "end_indices = [sum(sizes[:i+1]) for i in range(len(sizes))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "14fef404-9d92-4b71-854b-56090e10d3ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2048, 2128, 2208]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "begin_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f0dd7feb-09b2-462d-bdbc-37a95aa841e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net = PDETransformer(**config)\n",
    "net.to(\"cuda:0\")\n",
    "max_iter_adam = 2000\n",
    "lr = 1e-2\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "MSE = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5e12ea8d-7cab-409c-b7df-168475eae931",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ExponentialLR\n",
    "scheduler = CosineAnnealingLR(optimizer, max_iter_adam, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "eb9c8974-195c-45ce-b81f-92d92e75fb55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data(batch=8):\n",
    "    all_data_list = []\n",
    "    for b in range(batch):\n",
    "        collect_data = pinn_dataset.prepare_collection_data()\n",
    "\n",
    "        boundary_data_1 = np.random.uniform(0, 1, (80, 1))\n",
    "        start, end= geom[1]\n",
    "        boundary_data_1[:, 0] = boundary_data_1[:, 0] * (end - start) + start\n",
    "        new_col = np.full((boundary_data_1.shape[0], 1), -1.)\n",
    "        boundary_data_1 = np.hstack((new_col, boundary_data_1))\n",
    "        boundary_data_1 = torch.tensor(boundary_data_1, dtype=torch.float32)\n",
    "\n",
    "        boundary_data_2 = np.random.uniform(0, 1, (80, 1))\n",
    "        start, end= geom[1]\n",
    "        boundary_data_2[:, 0] = boundary_data_2[:, 0] * (end - start) + start\n",
    "        new_col = np.full((boundary_data_2.shape[0], 1), 1.)\n",
    "        boundary_data_2 = np.hstack((new_col, boundary_data_2))\n",
    "        boundary_data_2 = torch.tensor(boundary_data_2, dtype=torch.float32)\n",
    "\n",
    "        initial_data = np.random.uniform(0, 1, (160, 1))\n",
    "        start, end= geom[0]\n",
    "        initial_data[:, 0] = initial_data[:, 0] * (end - start) + start\n",
    "        new_col = np.full((initial_data.shape[0], 1), 0.)\n",
    "        initial_data = np.hstack((initial_data, new_col))\n",
    "        initial_data = torch.tensor(initial_data, dtype=torch.float32)\n",
    "\n",
    "        data_list = [collect_data,boundary_data_1, boundary_data_2, initial_data]\n",
    "        # data_list = [X_train,boundary_data_1, boundary_data_2, initial_data]\n",
    "        all_data = torch.cat(data_list, dim=0).unsqueeze(0)\n",
    "        \n",
    "        all_data_list.append(all_data)\n",
    "    \n",
    "    batch_data = torch.concat(all_data_list, dim=0)\n",
    "    sizes = [tensor.size(0) for tensor in data_list]\n",
    "    begin_indices = [sum(sizes[:i]) for i in range(len(sizes))]\n",
    "    end_indices = [sum(sizes[:i+1]) for i in range(len(sizes))]\n",
    "    return batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "653a66a1-af3a-4071-a6a9-c767983b132a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10 \t Loss:0.23006508 \t Loss_pde:0.01010154 \t Loss_bc_1:0.01668563 \t Loss_bc_2:0.01792203 \t Loss_ic:0.18535587\n",
      "Epoch:20 \t Loss:0.15993375 \t Loss_pde:0.02500059 \t Loss_bc_1:0.01351917 \t Loss_bc_2:0.01396872 \t Loss_ic:0.10744525\n",
      "Epoch:30 \t Loss:0.08302228 \t Loss_pde:0.02471663 \t Loss_bc_1:0.00737864 \t Loss_bc_2:0.00379564 \t Loss_ic:0.04713137\n",
      "Epoch:40 \t Loss:0.04270671 \t Loss_pde:0.01993411 \t Loss_bc_1:0.00247461 \t Loss_bc_2:0.00993042 \t Loss_ic:0.01036757\n",
      "Epoch:50 \t Loss:0.02109241 \t Loss_pde:0.01085589 \t Loss_bc_1:0.00392638 \t Loss_bc_2:0.00147624 \t Loss_ic:0.00483390\n",
      "Epoch:60 \t Loss:0.00919404 \t Loss_pde:0.00544563 \t Loss_bc_1:0.00056832 \t Loss_bc_2:0.00188784 \t Loss_ic:0.00129226\n",
      "Epoch:70 \t Loss:0.00676458 \t Loss_pde:0.00325604 \t Loss_bc_1:0.00171937 \t Loss_bc_2:0.00101040 \t Loss_ic:0.00077877\n",
      "Epoch:80 \t Loss:0.00356414 \t Loss_pde:0.00213791 \t Loss_bc_1:0.00038582 \t Loss_bc_2:0.00069345 \t Loss_ic:0.00034695\n",
      "Epoch:90 \t Loss:0.00601090 \t Loss_pde:0.00200363 \t Loss_bc_1:0.00142950 \t Loss_bc_2:0.00061404 \t Loss_ic:0.00196372\n",
      "Epoch:100 \t Loss:0.00425173 \t Loss_pde:0.00126854 \t Loss_bc_1:0.00041479 \t Loss_bc_2:0.00129630 \t Loss_ic:0.00127210\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m loss\u001b[38;5;241m=\u001b[39m loss_res\u001b[38;5;241m+\u001b[39mloss_bc_1\u001b[38;5;241m+\u001b[39mloss_bc_2\u001b[38;5;241m+\u001b[39mloss_ic \u001b[38;5;66;03m#+ loss_map\u001b[39;00m\n\u001b[1;32m     42\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 43\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     45\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/pit/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pit/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(1, max_iter_adam+1):\n",
    "    \n",
    "    batch_data = get_data(batch=4)\n",
    "    \n",
    "    input_data = batch_data.to('cuda:0') \n",
    "    indices = torch.arange(input_data.size(1))\n",
    "    shuffled_indices = indices[torch.randperm(len(indices))]\n",
    "    input_data = input_data[:, shuffled_indices]\n",
    "    \n",
    "    input_data.requires_grad=True\n",
    "    output, weight_list = net(input_data)\n",
    "    # print(output.shape)\n",
    "    \n",
    "#     loss_map=0\n",
    "    \n",
    "#     for weight in weight_list:\n",
    "#         weight=weight.squeeze(0)\n",
    "#         for index in range(weight.size(0)):\n",
    "#             loss_tmp = MSE(output.squeeze(0), torch.mm(weight[index], output.squeeze(0)))\n",
    "#             loss_map+=loss_tmp\n",
    "    \n",
    "    \n",
    "    \n",
    "    pde_results= pde(input_data, output)\n",
    "    loss_res = MSE(pde_results, torch.zeros_like(pde_results))\n",
    "    \n",
    "    output=output[:,shuffled_indices.argsort()]\n",
    "    input_data=input_data[:,shuffled_indices.argsort()]\n",
    "    \n",
    "    bc_error_1 = bc_1.error(input_data[:, begin_indices[1]:end_indices[1]],output[:, begin_indices[1]:end_indices[1]])\n",
    "    \n",
    "    \n",
    "    loss_bc_1 = MSE(bc_error_1, torch.zeros_like(bc_error_1))\n",
    "    \n",
    "    bc_error_2 = bc_2.error(input_data[:, begin_indices[2]:end_indices[2]],output[:, begin_indices[2]:end_indices[2]])\n",
    "    loss_bc_2 = MSE(bc_error_2, torch.zeros_like(bc_error_2))\n",
    "    \n",
    "    ic_error = ic.error(input_data[:, begin_indices[3]:end_indices[3]],output[:, begin_indices[3]:end_indices[3]])\n",
    "    loss_ic = MSE(ic_error, torch.zeros_like(ic_error))\n",
    "    \n",
    "    loss= loss_res+loss_bc_1+loss_bc_2+loss_ic #+ loss_map\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    if i%10 ==0:\n",
    "        print('Epoch:%d \\t Loss:%.8f \\t Loss_pde:%.8f \\t Loss_bc_1:%.8f \\t Loss_bc_2:%.8f \\t Loss_ic:%.8f'%(i,loss.detach().cpu().numpy(), loss_res.detach().cpu().numpy(), loss_bc_1.detach().cpu().numpy(), loss_bc_2.detach().cpu().numpy(), loss_ic.detach().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "97de772b-26f1-4e8c-9f21-9ecb5517635c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fixed_data(batch=8):\n",
    "    all_data_list = []\n",
    "    indices = torch.arange(X.shape[0])\n",
    "    shuffled_indices = indices[torch.randperm(len(indices))]\n",
    "    X_tensor = torch.tensor(X,dtype=torch.float32)[shuffled_indices]\n",
    "    all_collect_data = X_tensor.unsqueeze(0).reshape(batch, 3200,-1)\n",
    "    \n",
    "    for b in range(batch):   \n",
    "        collect_data = all_collect_data[b]\n",
    "        \n",
    "        boundary_data_1 = np.random.uniform(0, 1, (80, 1))\n",
    "        start, end= geom[1]\n",
    "        boundary_data_1[:, 0] = boundary_data_1[:, 0] * (end - start) + start\n",
    "        new_col = np.full((boundary_data_1.shape[0], 1), -1.)\n",
    "        boundary_data_1 = np.hstack((new_col, boundary_data_1))\n",
    "        boundary_data_1 = torch.tensor(boundary_data_1, dtype=torch.float32)\n",
    "\n",
    "        boundary_data_2 = np.random.uniform(0, 1, (80, 1))\n",
    "        start, end= geom[1]\n",
    "        boundary_data_2[:, 0] = boundary_data_2[:, 0] * (end - start) + start\n",
    "        new_col = np.full((boundary_data_2.shape[0], 1), 1.)\n",
    "        boundary_data_2 = np.hstack((new_col, boundary_data_2))\n",
    "        boundary_data_2 = torch.tensor(boundary_data_2, dtype=torch.float32)\n",
    "\n",
    "        initial_data = np.random.uniform(0, 1, (160, 1))\n",
    "        start, end= geom[0]\n",
    "        initial_data[:, 0] = initial_data[:, 0] * (end - start) + start\n",
    "        new_col = np.full((initial_data.shape[0], 1), 0.)\n",
    "        initial_data = np.hstack((initial_data, new_col))\n",
    "        initial_data = torch.tensor(initial_data, dtype=torch.float32)\n",
    "\n",
    "        data_list = [collect_data, boundary_data_1, boundary_data_2, initial_data]\n",
    "        # data_list = [X_train,boundary_data_1, boundary_data_2, initial_data]\n",
    "        \n",
    "        all_data = torch.cat(data_list, dim=0).unsqueeze(0)\n",
    "        \n",
    "\n",
    "        all_data_list.append(all_data)\n",
    "    \n",
    "    batch_data = torch.concat(all_data_list, dim=0)\n",
    "    sizes = [tensor.size(0) for tensor in data_list]\n",
    "    begin_indices = [sum(sizes[:i]) for i in range(len(sizes))]\n",
    "    end_indices = [sum(sizes[:i+1]) for i in range(len(sizes))]\n",
    "    return batch_data, shuffled_indices, begin_indices, end_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c7ef180a-9980-491b-bc1e-dd889669eba3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10 \t Loss:0.37086141 \t Loss_pde:0.02937155 \t Loss_bc_1:0.02224155 \t Loss_bc_2:0.04467606 \t Loss_ic:0.27457225\n",
      "Epoch:20 \t Loss:0.29234993 \t Loss_pde:0.01519601 \t Loss_bc_1:0.02117074 \t Loss_bc_2:0.04551912 \t Loss_ic:0.21046405\n",
      "Epoch:30 \t Loss:0.22837245 \t Loss_pde:0.01384033 \t Loss_bc_1:0.00459437 \t Loss_bc_2:0.04949094 \t Loss_ic:0.16044681\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m batch_data, shuffled_indices, begin_indices, end_indices \u001b[38;5;241m=\u001b[39mfixed_data(batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, max_iter_adam\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 5\u001b[0m     input_data \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda:0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[1;32m      6\u001b[0m     input_data\u001b[38;5;241m.\u001b[39mrequires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     output, weight_list \u001b[38;5;241m=\u001b[39m net(input_data)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_data, shuffled_indices, begin_indices, end_indices =fixed_data(batch=8)\n",
    "\n",
    "for i in range(1, max_iter_adam+1):\n",
    "    \n",
    "    input_data = batch_data.to('cuda:0') \n",
    "    input_data.requires_grad=True\n",
    "    output, weight_list = net(input_data)\n",
    "    # print(output.shape)\n",
    "    \n",
    "#     loss_map=0\n",
    "    \n",
    "#     for weight in weight_list:\n",
    "#         weight=weight.squeeze(0)\n",
    "#         for index in range(weight.size(0)):\n",
    "#             loss_tmp = MSE(output.squeeze(0), torch.mm(weight[index], output.squeeze(0)))\n",
    "#             loss_map+=loss_tmp\n",
    "    \n",
    "    \n",
    "    \n",
    "    pde_results= pde(input_data, output)\n",
    "    loss_res = MSE(pde_results, torch.zeros_like(pde_results))\n",
    "    \n",
    "    # output=output[:,shuffled_indices.argsort()]\n",
    "    # input_data=input_data[:,shuffled_indices.argsort()]\n",
    "    \n",
    "    bc_error_1 = bc_1.error(input_data[:, begin_indices[1]:end_indices[1]],output[:, begin_indices[1]:end_indices[1]])\n",
    "    \n",
    "    \n",
    "    loss_bc_1 = MSE(bc_error_1, torch.zeros_like(bc_error_1))\n",
    "    \n",
    "    bc_error_2 = bc_2.error(input_data[:, begin_indices[2]:end_indices[2]],output[:, begin_indices[2]:end_indices[2]])\n",
    "    loss_bc_2 = MSE(bc_error_2, torch.zeros_like(bc_error_2))\n",
    "    \n",
    "    ic_error = ic.error(input_data[:, begin_indices[3]:end_indices[3]],output[:, begin_indices[3]:end_indices[3]])\n",
    "    loss_ic = MSE(ic_error, torch.zeros_like(ic_error))\n",
    "    \n",
    "    loss= loss_res+loss_bc_1+loss_bc_2+loss_ic #+ loss_map\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    if i%10 ==0:\n",
    "        print('Epoch:%d \\t Loss:%.8f \\t Loss_pde:%.8f \\t Loss_bc_1:%.8f \\t Loss_bc_2:%.8f \\t Loss_ic:%.8f'%(i,loss.detach().cpu().numpy(), loss_res.detach().cpu().numpy(), loss_bc_1.detach().cpu().numpy(), loss_bc_2.detach().cpu().numpy(), loss_ic.detach().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e9351059-b7ce-4ed4-8b20-dc0b43827c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), './net_adam.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a01cb026-8f99-4d44-a70c-bbf0280e435e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net.eval()\n",
    "y_tensor = torch.tensor(y,dtype=torch.float32)[shuffled_indices]\n",
    "test_label = y_tensor.unsqueeze(0).reshape(8, 3200,-1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_pred,_ = net(batch_data.to(\"cuda:0\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "31ea794e-523e-42b6-b482-99c488fc7016",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    \n",
    "results = relative_l2_error(test_label, test_pred[:, :3200].squeeze(0).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5c18dfad-7b9f-4ac2-a968-b1f5a22f2fe4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0380)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "26995a7a-4ca0-49b7-881a-ba8607415298",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PDETransformer(\n",
       "  (feat_extract): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=64, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): Tanh()\n",
       "    (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (5): Tanh()\n",
       "    (6): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (7): Tanh()\n",
       "  )\n",
       "  (encoder_layers): ModuleList(\n",
       "    (0-1): 2 x PDETransformerEncoderLayer(\n",
       "      (attn): Attention(\n",
       "        (linears): ModuleList(\n",
       "          (0-2): 3 x Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (lr1): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (activation): Tanh()\n",
       "        (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (lr2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (regressor): PointwiseRegressor(\n",
       "    (ff): ModuleList(\n",
       "      (0-2): 3 x Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (out): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_lbfgs = PDETransformer(**config)\n",
    "net_lbfgs.load_state_dict(torch.load('./net_adam.pt'))\n",
    "net_lbfgs.to(\"cuda:0\")\n",
    "optimizer_lbfgs = torch.optim.LBFGS(\n",
    "            net_lbfgs.parameters(), \n",
    "            lr=1, \n",
    "            max_iter=100,  \n",
    "            history_size=10,\n",
    "            tolerance_grad=1e-8, \n",
    "            tolerance_change=0,\n",
    "            # line_search_fn='strong_wolfe',\n",
    "        )\n",
    "lbfgs_iter = 10000\n",
    "net_lbfgs.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "afdcc26d-7adf-4ce1-89b3-984e6b91ffa7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m epoch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m n_iter \u001b[38;5;241m-\u001b[39m prev_n_iter\n\u001b[1;32m     39\u001b[0m prev_n_iter \u001b[38;5;241m=\u001b[39m n_iter\n\u001b[0;32m---> 40\u001b[0m loss, loss_res, loss_bc_1, loss_bc_2, loss_ic \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m()\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, Loss:\u001b[39m\u001b[38;5;132;01m%.8f\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m Loss_pde:\u001b[39m\u001b[38;5;132;01m%.8f\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m Loss_bc_1:\u001b[39m\u001b[38;5;132;01m%.8f\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m Loss_bc_2:\u001b[39m\u001b[38;5;132;01m%.8f\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m Loss_ic:\u001b[39m\u001b[38;5;132;01m%.8f\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m%\u001b[39m(prev_n_iter, loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), loss_res\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), loss_bc_1\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), loss_bc_2\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), loss_ic\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "prev_n_iter = 0\n",
    "epoch = 0\n",
    "\n",
    "while prev_n_iter < lbfgs_iter:\n",
    "\n",
    "    def closure():\n",
    "        input_data = batch_data.to('cuda:0') \n",
    "        input_data.requires_grad=True\n",
    "        output, weight_list = net_lbfgs(input_data)\n",
    "        optimizer_lbfgs.zero_grad()\n",
    "        output, _ = net_lbfgs(input_data)\n",
    "\n",
    "\n",
    "        pde_results= pde(input_data, output)\n",
    "        loss_res = MSE(pde_results, torch.zeros_like(pde_results))\n",
    "\n",
    "        # output=output[:,shuffled_indices.argsort()]\n",
    "        # input_data=input_data[:,shuffled_indices.argsort()]\n",
    "\n",
    "        bc_error_1 = bc_1.error(input_data[:, begin_indices[1]:end_indices[1]],output[:, begin_indices[1]:end_indices[1]])\n",
    "\n",
    "        loss_bc_1 = MSE(bc_error_1, torch.zeros_like(bc_error_1))\n",
    "\n",
    "        bc_error_2 = bc_2.error(input_data[:, begin_indices[2]:end_indices[2]],output[:, begin_indices[2]:end_indices[2]])\n",
    "        loss_bc_2 = MSE(bc_error_2, torch.zeros_like(bc_error_2))\n",
    "\n",
    "        ic_error = ic.error(input_data[:, begin_indices[3]:end_indices[3]],output[:, begin_indices[3]:end_indices[3]])\n",
    "        loss_ic = MSE(ic_error, torch.zeros_like(ic_error))\n",
    "\n",
    "        loss= loss_res+loss_bc_1+loss_bc_2+loss_ic\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    optimizer_lbfgs.step(closure)\n",
    "    n_iter = optimizer_lbfgs.state_dict()[\"state\"][0]['n_iter']\n",
    "    if prev_n_iter == n_iter:\n",
    "        break\n",
    "    epoch += n_iter - prev_n_iter\n",
    "    prev_n_iter = n_iter\n",
    "    loss, loss_res, loss_bc_1, loss_bc_2, loss_ic = test()\n",
    "    print('Epoch: %d, Loss:%.8f \\t Loss_pde:%.8f \\t Loss_bc_1:%.8f \\t Loss_bc_2:%.8f \\t Loss_ic:%.8f'%(prev_n_iter, loss.detach().cpu().numpy(), loss_res.detach().cpu().numpy(), loss_bc_1.detach().cpu().numpy(), loss_bc_2.detach().cpu().numpy(), loss_ic.detach().cpu().numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "0cbe86a6-1321-4cf2-8b1a-b4a4c6431e12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 6400, 2])"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "9bc09837-dc14-436d-be07-9a8b4e39e9ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net.eval()\n",
    "indices = torch.arange(X.shape[0])\n",
    "shuffled_indices = indices[torch.randperm(len(indices))]\n",
    "\n",
    "test_data = torch.tensor(X,dtype=torch.float32).to('cuda:0').unsqueeze(0).reshape(4, 6400,-1)\n",
    "test_label = torch.tensor(y,dtype=torch.float32).to('cuda:0').unsqueeze(0).reshape(4, 6400,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "e8f1230e-81d0-4da9-9621-0308a429b6af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    test_pred,_ = net_lbfgs(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "191de428-8757-4af2-b8d9-d6ca8539d04d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 6400, 1])"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "c98905a7-0254-4361-a37c-0d9f650beba9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_pred = test_pred.view(100, 256,1).unsqueeze(0)\n",
    "# test_label = test_label.view(100, 256,1).unsqueeze(0)\n",
    "results = relative_l2_error(test_label, test_pred.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "2119b606-97f0-4246-9af3-e82b8d3a5585",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6387, device='cuda:0')"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "21fe7896-f4c2-4547-8146-4fa0d713d080",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PDETransformer(\n",
       "  (feat_extract): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=64, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): Tanh()\n",
       "    (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (5): Tanh()\n",
       "    (6): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (7): Tanh()\n",
       "  )\n",
       "  (encoder_layers): ModuleList(\n",
       "    (0-1): 2 x PDETransformerEncoderLayer(\n",
       "      (attn): Attention(\n",
       "        (linears): ModuleList(\n",
       "          (0-2): 3 x Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): FeedForward(\n",
       "        (lr1): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (activation): Tanh()\n",
       "        (lr2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (regressor): PointwiseRegressor(\n",
       "    (fc): Linear(in_features=66, out_features=64, bias=True)\n",
       "    (ff): ModuleList(\n",
       "      (0-2): 3 x Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (out): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_lbfgs = PDETransformer(**config)\n",
    "net_lbfgs.load_state_dict(torch.load('./net_adam.pt'))\n",
    "net_lbfgs.to(\"cuda:0\")\n",
    "optimizer_lbfgs = torch.optim.LBFGS(\n",
    "            net_lbfgs.parameters(), \n",
    "            lr=1, \n",
    "            max_iter=100,  \n",
    "            history_size=10,\n",
    "            tolerance_grad=1e-8, \n",
    "            tolerance_change=0,\n",
    "            # line_search_fn='strong_wolfe',\n",
    "        )\n",
    "lbfgs_iter = 10000\n",
    "net_lbfgs.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "05e33eb4-f271-4184-9c72-37e250851b26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    net_lbfgs.eval()\n",
    "    \n",
    "    batch_data = get_data(batch=1)\n",
    "    \n",
    "    input_data = batch_data.to('cuda:0') \n",
    "    indices = torch.arange(input_data.size(1))\n",
    "    shuffled_indices = indices[torch.randperm(len(indices))]\n",
    "    input_data = input_data[:, shuffled_indices]\n",
    "    \n",
    "    input_data.requires_grad=True\n",
    "    output, _ = net_lbfgs(input_data)\n",
    "    \n",
    "    pde_results= pde(input_data, output)\n",
    "    loss_res = MSE(pde_results, torch.zeros_like(pde_results))\n",
    "    \n",
    "    \n",
    "    output=output[:,shuffled_indices.argsort()]\n",
    "    input_data=input_data[:,shuffled_indices.argsort()]\n",
    "    \n",
    "    bc_error_1 = bc_1.error(input_data[:, begin_indices[1]:end_indices[1]],output[:, begin_indices[1]:end_indices[1]])\n",
    "    \n",
    "    \n",
    "    loss_bc_1 = MSE(bc_error_1, torch.zeros_like(bc_error_1))\n",
    "    \n",
    "    bc_error_2 = bc_2.error(input_data[:, begin_indices[2]:end_indices[2]],output[:, begin_indices[2]:end_indices[2]])\n",
    "    loss_bc_2 = MSE(bc_error_2, torch.zeros_like(bc_error_2))\n",
    "    \n",
    "    ic_error = ic.error(input_data[:, begin_indices[3]:end_indices[3]],output[:, begin_indices[3]:end_indices[3]])\n",
    "    loss_ic = MSE(ic_error, torch.zeros_like(ic_error))\n",
    "\n",
    "    loss= loss_res+loss_bc_1+loss_bc_2+loss_ic\n",
    "    return loss, loss_res, loss_bc_1, loss_bc_2, loss_ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "62faf9c1-f9f3-4c46-b06e-982f37091814",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, Loss:0.00127215 \t Loss_pde:0.00021410 \t Loss_bc_1:0.00034431 \t Loss_bc_2:0.00027663 \t Loss_ic:0.00043711\n",
      "Epoch: 200, Loss:0.00225896 \t Loss_pde:0.00024492 \t Loss_bc_1:0.00070259 \t Loss_bc_2:0.00042404 \t Loss_ic:0.00088741\n",
      "Epoch: 300, Loss:0.00082792 \t Loss_pde:0.00007870 \t Loss_bc_1:0.00024741 \t Loss_bc_2:0.00016381 \t Loss_ic:0.00033800\n",
      "Epoch: 400, Loss:0.00028635 \t Loss_pde:0.00008336 \t Loss_bc_1:0.00008416 \t Loss_bc_2:0.00004706 \t Loss_ic:0.00007177\n",
      "Epoch: 500, Loss:0.00031579 \t Loss_pde:0.00008105 \t Loss_bc_1:0.00008163 \t Loss_bc_2:0.00005435 \t Loss_ic:0.00009877\n",
      "Epoch: 600, Loss:0.00017697 \t Loss_pde:0.00008675 \t Loss_bc_1:0.00000355 \t Loss_bc_2:0.00005524 \t Loss_ic:0.00003142\n",
      "Epoch: 700, Loss:0.00029267 \t Loss_pde:0.00005691 \t Loss_bc_1:0.00008175 \t Loss_bc_2:0.00004795 \t Loss_ic:0.00010605\n",
      "Epoch: 800, Loss:0.00071410 \t Loss_pde:0.00007356 \t Loss_bc_1:0.00025051 \t Loss_bc_2:0.00011804 \t Loss_ic:0.00027198\n",
      "Epoch: 900, Loss:0.00012797 \t Loss_pde:0.00004456 \t Loss_bc_1:0.00003733 \t Loss_bc_2:0.00001122 \t Loss_ic:0.00003486\n",
      "Epoch: 1000, Loss:0.00173575 \t Loss_pde:0.00012552 \t Loss_bc_1:0.00069357 \t Loss_bc_2:0.00032045 \t Loss_ic:0.00059620\n",
      "Epoch: 1100, Loss:0.00010757 \t Loss_pde:0.00003940 \t Loss_bc_1:0.00000711 \t Loss_bc_2:0.00003516 \t Loss_ic:0.00002590\n",
      "Epoch: 1200, Loss:0.00029848 \t Loss_pde:0.00004791 \t Loss_bc_1:0.00008067 \t Loss_bc_2:0.00008138 \t Loss_ic:0.00008852\n",
      "Epoch: 1300, Loss:0.00041762 \t Loss_pde:0.00001995 \t Loss_bc_1:0.00016289 \t Loss_bc_2:0.00007995 \t Loss_ic:0.00015483\n",
      "Epoch: 1400, Loss:0.00004701 \t Loss_pde:0.00002113 \t Loss_bc_1:0.00000694 \t Loss_bc_2:0.00000942 \t Loss_ic:0.00000953\n",
      "Epoch: 1500, Loss:0.00054327 \t Loss_pde:0.00004002 \t Loss_bc_1:0.00022205 \t Loss_bc_2:0.00009118 \t Loss_ic:0.00019002\n",
      "Epoch: 1600, Loss:0.00031959 \t Loss_pde:0.00006176 \t Loss_bc_1:0.00013155 \t Loss_bc_2:0.00003799 \t Loss_ic:0.00008830\n",
      "Epoch: 1700, Loss:0.00053666 \t Loss_pde:0.00003801 \t Loss_bc_1:0.00020960 \t Loss_bc_2:0.00009748 \t Loss_ic:0.00019158\n",
      "Epoch: 1800, Loss:0.00005864 \t Loss_pde:0.00001695 \t Loss_bc_1:0.00002225 \t Loss_bc_2:0.00000668 \t Loss_ic:0.00001276\n",
      "Epoch: 1900, Loss:0.00006918 \t Loss_pde:0.00002332 \t Loss_bc_1:0.00001804 \t Loss_bc_2:0.00001021 \t Loss_ic:0.00001761\n",
      "Epoch: 2000, Loss:0.00048974 \t Loss_pde:0.00002580 \t Loss_bc_1:0.00019661 \t Loss_bc_2:0.00010243 \t Loss_ic:0.00016489\n",
      "Epoch: 2100, Loss:0.00010004 \t Loss_pde:0.00004741 \t Loss_bc_1:0.00002972 \t Loss_bc_2:0.00000191 \t Loss_ic:0.00002101\n",
      "Epoch: 2200, Loss:0.00023870 \t Loss_pde:0.00002545 \t Loss_bc_1:0.00011651 \t Loss_bc_2:0.00002820 \t Loss_ic:0.00006855\n",
      "Epoch: 2300, Loss:0.00003276 \t Loss_pde:0.00002392 \t Loss_bc_1:0.00000076 \t Loss_bc_2:0.00000306 \t Loss_ic:0.00000502\n",
      "Epoch: 2400, Loss:0.00003593 \t Loss_pde:0.00001161 \t Loss_bc_1:0.00001294 \t Loss_bc_2:0.00000207 \t Loss_ic:0.00000931\n",
      "Epoch: 2500, Loss:0.00003866 \t Loss_pde:0.00001598 \t Loss_bc_1:0.00000289 \t Loss_bc_2:0.00000691 \t Loss_ic:0.00001288\n",
      "Epoch: 2600, Loss:0.00015594 \t Loss_pde:0.00004305 \t Loss_bc_1:0.00005515 \t Loss_bc_2:0.00000353 \t Loss_ic:0.00005420\n",
      "Epoch: 2700, Loss:0.00003746 \t Loss_pde:0.00001546 \t Loss_bc_1:0.00000747 \t Loss_bc_2:0.00000632 \t Loss_ic:0.00000821\n",
      "Epoch: 2800, Loss:0.00003068 \t Loss_pde:0.00001945 \t Loss_bc_1:0.00000058 \t Loss_bc_2:0.00000703 \t Loss_ic:0.00000363\n",
      "Epoch: 2900, Loss:0.00003349 \t Loss_pde:0.00001059 \t Loss_bc_1:0.00000724 \t Loss_bc_2:0.00000500 \t Loss_ic:0.00001067\n",
      "Epoch: 3000, Loss:0.00031170 \t Loss_pde:0.00003116 \t Loss_bc_1:0.00010123 \t Loss_bc_2:0.00007350 \t Loss_ic:0.00010582\n",
      "Epoch: 3100, Loss:0.00044719 \t Loss_pde:0.00003071 \t Loss_bc_1:0.00015853 \t Loss_bc_2:0.00009547 \t Loss_ic:0.00016248\n",
      "Epoch: 3200, Loss:0.00087485 \t Loss_pde:0.00006238 \t Loss_bc_1:0.00028510 \t Loss_bc_2:0.00022148 \t Loss_ic:0.00030589\n",
      "Epoch: 3300, Loss:0.00022723 \t Loss_pde:0.00002447 \t Loss_bc_1:0.00009965 \t Loss_bc_2:0.00003171 \t Loss_ic:0.00007140\n",
      "Epoch: 3400, Loss:0.00003120 \t Loss_pde:0.00001171 \t Loss_bc_1:0.00000219 \t Loss_bc_2:0.00001160 \t Loss_ic:0.00000571\n",
      "Epoch: 3500, Loss:0.00015678 \t Loss_pde:0.00002705 \t Loss_bc_1:0.00003923 \t Loss_bc_2:0.00004515 \t Loss_ic:0.00004535\n",
      "Epoch: 3600, Loss:0.00053430 \t Loss_pde:0.00001595 \t Loss_bc_1:0.00017751 \t Loss_bc_2:0.00015470 \t Loss_ic:0.00018614\n",
      "Epoch: 3700, Loss:0.00007765 \t Loss_pde:0.00002053 \t Loss_bc_1:0.00000833 \t Loss_bc_2:0.00002893 \t Loss_ic:0.00001986\n",
      "Epoch: 3800, Loss:0.00060747 \t Loss_pde:0.00004923 \t Loss_bc_1:0.00022305 \t Loss_bc_2:0.00011419 \t Loss_ic:0.00022101\n",
      "Epoch: 3900, Loss:0.00001788 \t Loss_pde:0.00000884 \t Loss_bc_1:0.00000415 \t Loss_bc_2:0.00000142 \t Loss_ic:0.00000347\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[258], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[0;32m---> 37\u001b[0m \u001b[43moptimizer_lbfgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m n_iter \u001b[38;5;241m=\u001b[39m optimizer_lbfgs\u001b[38;5;241m.\u001b[39mstate_dict()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_iter\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prev_n_iter \u001b[38;5;241m==\u001b[39m n_iter:\n",
      "File \u001b[0;32m~/anaconda3/envs/pit/lib/python3.8/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pit/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pit/lib/python3.8/site-packages/torch/optim/lbfgs.py:438\u001b[0m, in \u001b[0;36mLBFGS.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_iter \u001b[38;5;241m!=\u001b[39m max_iter:\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;66;03m# re-evaluate function only if not in last iteration\u001b[39;00m\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;66;03m# the reason we do this: in a stochastic setting,\u001b[39;00m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;66;03m# no use to re-evaluate that function here\u001b[39;00m\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 438\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    439\u001b[0m     flat_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gather_flat_grad()\n\u001b[1;32m    440\u001b[0m     opt_cond \u001b[38;5;241m=\u001b[39m flat_grad\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m tolerance_grad\n",
      "File \u001b[0;32m~/anaconda3/envs/pit/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[258], line 17\u001b[0m, in \u001b[0;36mclosure\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m optimizer_lbfgs\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     14\u001b[0m output, _ \u001b[38;5;241m=\u001b[39m net_lbfgs(input_data)\n\u001b[0;32m---> 17\u001b[0m pde_results\u001b[38;5;241m=\u001b[39m \u001b[43mpde\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m loss_res \u001b[38;5;241m=\u001b[39m MSE(pde_results, torch\u001b[38;5;241m.\u001b[39mzeros_like(pde_results))\n\u001b[1;32m     20\u001b[0m output\u001b[38;5;241m=\u001b[39moutput[:,shuffled_indices\u001b[38;5;241m.\u001b[39margsort()]\n",
      "Cell \u001b[0;32mIn[234], line 2\u001b[0m, in \u001b[0;36mpde\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpde\u001b[39m(x, y):\n\u001b[0;32m----> 2\u001b[0m     dy_x \u001b[38;5;241m=\u001b[39m \u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][:, :, \u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      3\u001b[0m     dy_t \u001b[38;5;241m=\u001b[39m grad(y, x, grad_outputs\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mones_like(y), retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m][:, :, \u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m      4\u001b[0m     dy_xx \u001b[38;5;241m=\u001b[39m grad(dy_x, x, grad_outputs\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mones_like(y), retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m][:, :, \u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/pit/lib/python3.8/site-packages/torch/autograd/__init__.py:303\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    304\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prev_n_iter = 0\n",
    "epoch = 0\n",
    "\n",
    "while prev_n_iter < lbfgs_iter:\n",
    "    batch_data = get_data(batch=4) \n",
    "    def closure():\n",
    "        input_data = batch_data.to('cuda:0') \n",
    "        indices = torch.arange(input_data.size(1))\n",
    "        shuffled_indices = indices[torch.randperm(len(indices))]\n",
    "        input_data = input_data[:, shuffled_indices]\n",
    "\n",
    "        input_data.requires_grad=True\n",
    "        optimizer_lbfgs.zero_grad()\n",
    "        output, _ = net_lbfgs(input_data)\n",
    "\n",
    "\n",
    "        pde_results= pde(input_data, output)\n",
    "        loss_res = MSE(pde_results, torch.zeros_like(pde_results))\n",
    "\n",
    "        output=output[:,shuffled_indices.argsort()]\n",
    "        input_data=input_data[:,shuffled_indices.argsort()]\n",
    "\n",
    "        bc_error_1 = bc_1.error(input_data[:, begin_indices[1]:end_indices[1]],output[:, begin_indices[1]:end_indices[1]])\n",
    "\n",
    "        loss_bc_1 = MSE(bc_error_1, torch.zeros_like(bc_error_1))\n",
    "\n",
    "        bc_error_2 = bc_2.error(input_data[:, begin_indices[2]:end_indices[2]],output[:, begin_indices[2]:end_indices[2]])\n",
    "        loss_bc_2 = MSE(bc_error_2, torch.zeros_like(bc_error_2))\n",
    "\n",
    "        ic_error = ic.error(input_data[:, begin_indices[3]:end_indices[3]],output[:, begin_indices[3]:end_indices[3]])\n",
    "        loss_ic = MSE(ic_error, torch.zeros_like(ic_error))\n",
    "\n",
    "        loss= loss_res+loss_bc_1+loss_bc_2+loss_ic\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    optimizer_lbfgs.step(closure)\n",
    "    n_iter = optimizer_lbfgs.state_dict()[\"state\"][0]['n_iter']\n",
    "    if prev_n_iter == n_iter:\n",
    "        break\n",
    "    epoch += n_iter - prev_n_iter\n",
    "    prev_n_iter = n_iter\n",
    "    loss, loss_res, loss_bc_1, loss_bc_2, loss_ic = test()\n",
    "    print('Epoch: %d, Loss:%.8f \\t Loss_pde:%.8f \\t Loss_bc_1:%.8f \\t Loss_bc_2:%.8f \\t Loss_ic:%.8f'%(prev_n_iter, loss.detach().cpu().numpy(), loss_res.detach().cpu().numpy(), loss_bc_1.detach().cpu().numpy(), loss_bc_2.detach().cpu().numpy(), loss_ic.detach().cpu().numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54030137-2a27-4815-b4a3-d69895bf71d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efe7a39-01a0-46ee-88fb-7495f5b16c7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
